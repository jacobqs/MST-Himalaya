{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import datetime\n",
    "import numpy as np\n",
    "from metpy.calc import relative_humidity_from_specific_humidity\n",
    "from metpy.units import units\n",
    "from pyproj import Proj, Transformer, CRS\n",
    "from pyproj.aoi import AreaOfInterest\n",
    "from pyproj.database import query_utm_crs_info\n",
    "import rasterio\n",
    "from shapely.geometry import Point\n",
    "import geopandas as gpd\n",
    "import os\n",
    "import glob\n",
    "from zipfile import ZipFile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "toposcale_path = '/Users/jacobqs/Documents/MST Himalaya/MST-Himalaya/shyft_workspace_copy/shyft_workspace/shyft-data/netcdf/orchestration-testdata/TopoScale/TopoSCALE_BG.nc'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = xr.open_mfdataset(toposcale_path)\n",
    "ds_raw = ds.copy()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dimensions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Time, t (149016)\n",
    "* Clusters, clusters (200)\n",
    "* Pixels, pixels (571612)\n",
    "* Latitude, latitude (1560)\n",
    "* Longitude, longitude (1380)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data are hourly starting on the 01.01.1999 00:00 UTC and running to 31.12.2015 23:00 UTC"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coordinates (EPSG:4326)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Latitude of cluster centroids [degrees N], latc (clusters)\n",
    "* Longitude of cluster centroids [degrees E], lonc (clusters)\n",
    "* Latitude of catchment pixels [degrees N], latp (pixels)\n",
    "* Longitude of catchment pixels [degrees E], lonp (pixels)\n",
    "* Latitude grid [degrees N], latg (latitude)\n",
    "* Longitude grid [degrees E], long (longitude)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variables and their dimensions and scaling factors\n",
    "\n",
    "* 2 m air temperature, T (deg C), (t, clusters): 0.01\n",
    "* Wind speed, U (m/s), (t, clusters): 0.01\n",
    "* Specific humidity, q (kg/kg), (t, clusters): 10**-6\n",
    "* Incoming atmospheric shortwave radiation, SW (W/m^2), (t, clusters): 0.1\n",
    "* Total precipitation, P (mm h-1), (t, clusters): 0.01\n",
    "* Surface pressure, ps (Pa), (t, clusters): 100\n",
    "* Cluster number of each pixel, cn (), (pixels)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling factors\n",
    "\n",
    "T_sf = 0.01\n",
    "P_sf = 0.01\n",
    "U_sf = 0.01\n",
    "q_sf = 10**-6\n",
    "SW_sf = 0.1\n",
    "ps_sf = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling of variables\n",
    "\n",
    "## Temperature\n",
    "T_scaled = ds.T * T_sf\n",
    "ds = ds.assign(T_scaled = T_scaled)\n",
    "\n",
    "# Precipitation\n",
    "P_scaled = ds.P * P_sf\n",
    "ds = ds.assign(P_scaled = P_scaled)\n",
    "\n",
    "# Wind\n",
    "U_scaled = ds.U * U_sf\n",
    "ds = ds.assign(U_scaled = U_scaled)\n",
    "\n",
    "# Specific humidity\n",
    "q_scaled = ds.q * q_sf\n",
    "ds = ds.assign(q_scaled = q_scaled)\n",
    "\n",
    "# Shortwave down\n",
    "\n",
    "SW_scaled = ds.SW * SW_sf\n",
    "ds = ds.assign(SW_scaled = SW_scaled)\n",
    "\n",
    "# Pressure\n",
    "ps_scaled = ds.ps * ps_sf\n",
    "ds = ds.assign(ps_scaled = ps_scaled)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting timestamps, t, to datetime64\n",
    "dates = []\n",
    "start_date = datetime.datetime(1999,1,1)\n",
    "sec_since_epoch = datetime.datetime.timestamp(start_date)\n",
    "delta_seconds = ds.t.values * 60 * 60\n",
    "\n",
    "for i in delta_seconds:\n",
    "    time = sec_since_epoch + i\n",
    "    time = np.datetime64(datetime.datetime.fromtimestamp(time))\n",
    "    dates.append(time)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Switch dimensions from t to time\n",
    "\n",
    "##Create a new dimension called 'time'\n",
    "ds = ds.assign_coords(time=dates)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a new dataset where variables have the new time dimension\n",
    "ds_new = xr.Dataset(\n",
    "    data_vars = dict(\n",
    "        T = (['time', 'clusters'], ds.T.values),\n",
    "        U = (['time', 'clusters'], ds.U.values),\n",
    "        q = (['time', 'clusters'], ds.q.values),\n",
    "        LW = (['time', 'clusters'], ds.LW.values),\n",
    "        SW = (['time', 'clusters'], ds.SW.values),\n",
    "        P = (['time', 'clusters'], ds.P.values),\n",
    "        ps = (['time', 'clusters'], ds.ps.values),\n",
    "        latc = (['clusters'], ds.latc.values),\n",
    "        lonc = (['clusters'], ds.lonc.values),\n",
    "        latp = (['pixels'], ds.latp.values),\n",
    "        lonp = (['pixels'], ds.lonp.values),\n",
    "        cn = (['pixels'], ds.cn.values),\n",
    "        latg = (['latitude'], ds.latg.values),\n",
    "        long = (['longitude'], ds.long.values),\n",
    "        mask = (['longitude', 'latitude'], ds.mask.values),\n",
    "        T_scaled = (['time', 'clusters'], ds.T_scaled.values),\n",
    "        P_scaled = (['time', 'clusters'], ds.P_scaled.values),\n",
    "        U_scaled = (['time', 'clusters'], ds.U_scaled.values),\n",
    "        q_scaled = (['time', 'clusters'], ds.q_scaled.values),\n",
    "        SW_scaled = (['time', 'clusters'], ds.SW_scaled.values),\n",
    "        ps_scaled = (['time', 'clusters'], ds.ps_scaled.values),\n",
    "    ),\n",
    "    coords = dict(\n",
    "        time = (['time'], dates)\n",
    "    )\n",
    "    \n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing data for Shyft"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shyft needs:\n",
    "* Temperature (deg c)\n",
    "* Precipitation (mm h^-1)\n",
    "* SW radiation (W m^-2)\n",
    "* Relative humidity (kg kg^-1)\n",
    "* Wind speed (m s^-1)\n",
    "\n",
    "The new dataset already has:\n",
    "* Temperature (deg C)\n",
    "* Wind speed (m s^-1)\n",
    "* SW radiation (W m^-2)\n",
    "* Precipitation (mm h^-1)\n",
    "\n",
    "Therefore we only need to get relative humidity. This can be done using the MetPy package"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting pressure from Pa to hPa and getting relative humidity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To use the relative humidity function from MetPy, the surface pressure must be converted from Pa to hPa\n",
    "\n",
    "ps_scaled_hpa = ds_new.ps_scaled * 0.01\n",
    "ds_new = ds_new.assign(ps_scaled_hpa = ps_scaled_hpa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting relative humidity using the MetPy package. \n",
    "# NB! Values are in percent and needs to be multiplied by 0.01 to get fraction\n",
    "rel_hum_values = relative_humidity_from_specific_humidity(ds_new.ps_scaled_hpa.values * units.hPa, \n",
    "                                                        ds.T_scaled.values * units.degC, \n",
    "                                                        ds.q_scaled.values).to('percent').magnitude * 0.01"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting the altitude for each cluster point"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have all the forcing variables for Shyft. We also need altitude z and convert the coordinates to X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we extract the longitudes and latitudes for the clusters \n",
    "# (variables have dimension ('time', 'clusters'))\n",
    "\n",
    "lonc = ds_new.lonc.values\n",
    "latc = ds_new.latc.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making a list with a tuple of all (lon, lat) points\n",
    "\n",
    "lonc_latc_points = []\n",
    "\n",
    "for i in range(len(lonc)):\n",
    "    lonc_latc_points.append(Point((lonc[i], latc[i])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a GeoPandas dataframe with all the clusters and points\n",
    "gdf = gpd.GeoDataFrame(np.arange(len(lonc_latc_points)), geometry = lonc_latc_points, crs = 4326)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a list of all the points (lon, lat)\n",
    "coord_list = [(x,y) for x,y in zip(gdf['geometry'].x , gdf['geometry'].y)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To get altitude DEM tiles from USGS are merged into one DEM using QGIS and CRS: 4326\n",
    "\n",
    "## Opening this merged DEM\n",
    "\n",
    "dem_dataset = rasterio.open('/Users/jacobqs/Documents/MST Himalaya/qgis/DEM/merged_DEM_central_himalaya.tif')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample the coordinate points in coord_list in the DEM file\n",
    "gdf['value'] = [x for x in dem_dataset.sample(coord_list)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting a list of all z values\n",
    "z = []\n",
    "for i in gdf['value'].values:\n",
    "    z.append(i[0])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have all the z values for each cluster in the GeoPandas dataframe, gdf"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting from lat, lon to x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Making a pyproj object for UTM Zone 45N\n",
    "\n",
    "crs_4326 = CRS.from_epsg(4326) # Lat-lon\n",
    "crs_32645 = CRS.from_epsg(32645) # CRS \n",
    "\n",
    "## Create transformer to convert from CRS to CRS\n",
    "\n",
    "transformer = Transformer.from_crs(crs_4326, crs_32645, always_xy = True)\n",
    "\n",
    "x, y = transformer.transform(lonc, latc)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making new netcdf files for Shyft"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making a new dataset for each variable to make smaller sized files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [],
   "source": [
    "home_dir = os.getenv(\"HOME\")\n",
    "wfde5_path = home_dir + '/Documents/MST Himalaya/WFDE5/'\n",
    "cell_data_path = glob.glob(wfde5_path + 'cell_data/*.nc')\n",
    "working_dir = '/Users/jacobqs/Documents/MST Himalaya/MST-Himalaya/'\n",
    "test_path = working_dir + '/shyft_workspace/shyft-data/netcdf/orchestration-testdata/'\n",
    "temp_test_path = test_path + 'temperature.nc'\n",
    "relhum_test_path = test_path + 'relative_humidity.nc'\n",
    "precip_test_path = test_path + 'precipitation.nc'\n",
    "swdown_test_path = test_path + 'radiation.nc'\n",
    "wind_test_path = test_path + 'wind_speed.nc'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_test = xr.open_mfdataset(temp_test_path)\n",
    "relhum_test = xr.open_mfdataset(relhum_test_path)\n",
    "precip_test = xr.open_mfdataset(precip_test_path)\n",
    "swdown_test = xr.open_mfdataset(swdown_test_path)\n",
    "wind_test = xr.open_mfdataset(wind_test_path)\n",
    "cell_data_test = xr.open_mfdataset(cell_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_temperature = xr.Dataset(\n",
    "    data_vars = dict(\n",
    "        series_name = (['station'], np.arange(len(ds_new.clusters))),\n",
    "        crs = ((), np.array(-2147483647, dtype= 'int32')),\n",
    "        temperature = (['time', 'station'], ds_new.T_scaled.values)\n",
    "    ),\n",
    "    coords = dict(\n",
    "        time = (['time'], ds_new.time.values),\n",
    "        x = (['station'], x),\n",
    "        y = (['station'], y),\n",
    "        z = (['station'], z)\n",
    "    )  \n",
    ")\n",
    "\n",
    "ds_precipitation = xr.Dataset(\n",
    "    data_vars = dict(\n",
    "        series_name = (['station'], np.arange(len(ds_new.clusters))),\n",
    "        crs = ((), np.array(-2147483647, dtype= 'int32')),\n",
    "        precipitation = (['time', 'station'], ds_new.P_scaled.values)\n",
    "    ),\n",
    "    coords = dict(\n",
    "        time = (['time'], ds_new.time.values),\n",
    "        x = (['station'], x),\n",
    "        y = (['station'], y),\n",
    "        z = (['station'], z)\n",
    "    )  \n",
    ")\n",
    "\n",
    "ds_wind = xr.Dataset(\n",
    "    data_vars = dict(\n",
    "        series_name = (['station'], np.arange(len(ds_new.clusters))),\n",
    "        crs = ((), np.array(-2147483647, dtype= 'int32')),\n",
    "        wind_speed = (['time', 'station'], ds_new.U_scaled.values)\n",
    "    ),\n",
    "    coords = dict(\n",
    "        time = (['time'], ds_new.time.values),\n",
    "        x = (['station'], x),\n",
    "        y = (['station'], y),\n",
    "        z = (['station'], z)\n",
    "    )  \n",
    ")\n",
    "\n",
    "ds_radiation = xr.Dataset(\n",
    "    data_vars = dict(\n",
    "        series_name = (['station'], np.arange(len(ds_new.clusters))),\n",
    "        crs = ((), np.array(-2147483647, dtype= 'int32')),\n",
    "        global_radiation = (['time', 'station'], ds_new.SW_scaled.values)\n",
    "    ),\n",
    "    coords = dict(\n",
    "        time = (['time'], ds_new.time.values),\n",
    "        x = (['station'], x),\n",
    "        y = (['station'], y),\n",
    "        z = (['station'], z)\n",
    "    )  \n",
    ")\n",
    "\n",
    "ds_relhum = xr.Dataset(\n",
    "    data_vars = dict(\n",
    "        series_name = (['station'], np.arange(len(ds_new.clusters))),\n",
    "        crs = ((), np.array(-2147483647, dtype= 'int32')),\n",
    "        relative_humidity = (['time', 'station'], rel_hum_values)\n",
    "    ),\n",
    "    coords = dict(\n",
    "        time = (['time'], ds_new.time.values),\n",
    "        x = (['station'], x),\n",
    "        y = (['station'], y),\n",
    "        z = (['station'], z)\n",
    "    )  \n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have five datasets (1 for each variables) ready to be made into netcdf formats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a dictionary containing all variables with one dataset for each year\n",
    "years = ['1999', '2000', '2001', '2002', '2003', '2004', '2005', '2006', '2007', '2008', '2009', \n",
    "        '2010', '2011', '2012', '2013', '2014', '2015']\n",
    "datasets_ds = dict(\n",
    "                temperature = ds_temperature,\n",
    "                precipitation = ds_precipitation,\n",
    "                wind = ds_wind,\n",
    "                radiation = ds_radiation,\n",
    "                relative_humidity = ds_relhum\n",
    ")\n",
    "\n",
    "datasets_years = dict()\n",
    "\n",
    "for name in datasets_ds.keys():\n",
    "    datasets_years[name] = dict()\n",
    "    for year in years:\n",
    "        ds_year = datasets_ds[name].sel(time = year)\n",
    "        datasets_years[name][year] = ds_year\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a net cdf file for eachyear for each variable\n",
    "\n",
    "for i in datasets_years.keys():\n",
    "    for j in datasets_years[i].keys():\n",
    "        datasets_years[i][j].to_netcdf(home_dir + '/Downloads/' + f'{i}_' + f'{j}.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ZIP file created\n",
      "ZIP file created\n",
      "ZIP file created\n",
      "ZIP file created\n",
      "ZIP file created\n",
      "ZIP file created\n",
      "ZIP file created\n",
      "ZIP file created\n",
      "ZIP file created\n",
      "ZIP file created\n",
      "ZIP file created\n",
      "ZIP file created\n",
      "ZIP file created\n",
      "ZIP file created\n",
      "ZIP file created\n",
      "ZIP file created\n",
      "ZIP file created\n",
      "ZIP file created\n",
      "ZIP file created\n",
      "ZIP file created\n",
      "ZIP file created\n",
      "ZIP file created\n",
      "ZIP file created\n",
      "ZIP file created\n",
      "ZIP file created\n",
      "ZIP file created\n",
      "ZIP file created\n",
      "ZIP file created\n",
      "ZIP file created\n",
      "ZIP file created\n",
      "ZIP file created\n",
      "ZIP file created\n",
      "ZIP file created\n",
      "ZIP file created\n",
      "ZIP file created\n",
      "ZIP file created\n",
      "ZIP file created\n",
      "ZIP file created\n",
      "ZIP file created\n",
      "ZIP file created\n",
      "ZIP file created\n",
      "ZIP file created\n",
      "ZIP file created\n",
      "ZIP file created\n",
      "ZIP file created\n",
      "ZIP file created\n",
      "ZIP file created\n",
      "ZIP file created\n",
      "ZIP file created\n",
      "ZIP file created\n",
      "ZIP file created\n",
      "ZIP file created\n",
      "ZIP file created\n",
      "ZIP file created\n",
      "ZIP file created\n",
      "ZIP file created\n",
      "ZIP file created\n",
      "ZIP file created\n",
      "ZIP file created\n",
      "ZIP file created\n",
      "ZIP file created\n",
      "ZIP file created\n",
      "ZIP file created\n",
      "ZIP file created\n",
      "ZIP file created\n",
      "ZIP file created\n",
      "ZIP file created\n",
      "ZIP file created\n",
      "ZIP file created\n",
      "ZIP file created\n",
      "ZIP file created\n",
      "ZIP file created\n",
      "ZIP file created\n",
      "ZIP file created\n",
      "ZIP file created\n",
      "ZIP file created\n",
      "ZIP file created\n",
      "ZIP file created\n",
      "ZIP file created\n",
      "ZIP file created\n",
      "ZIP file created\n",
      "ZIP file created\n",
      "ZIP file created\n",
      "ZIP file created\n",
      "ZIP file created\n"
     ]
    }
   ],
   "source": [
    "# Create zip files for each netcdf file\n",
    "\n",
    "for i in datasets_years.keys():\n",
    "    for j in datasets_years[i].keys():\n",
    "\n",
    "      # Create a ZipFile Object\n",
    "      with ZipFile(f'/Users/jacobqs/Downloads/{i}_{j}.nc.zip', 'w') as zip_object:\n",
    "         # Adding files that need to be zipped\n",
    "         zip_object.write(f'/Users/jacobqs/Downloads/{i}_{j}.nc')\n",
    "\n",
    "\n",
    "      # Check to see if the zip file is created\n",
    "      if os.path.exists(f'/Users/jacobqs/Downloads/{i}_{j}.nc.zip'):\n",
    "         print(\"ZIP file created\")\n",
    "      else:\n",
    "         print(\"ZIP file not created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geohyd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b869024099f15872fcd4e15ccfb76c4e0e11665f80101cb27bb68f928ec91781"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
